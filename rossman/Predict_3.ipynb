{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import random\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets, linear_model, cross_validation, grid_search, svm, ensemble, kernel_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rmspe(y, yhat):\n",
    "    y = y.astype('float')\n",
    "    yhat = yhat.astype('float')\n",
    "    inner = ((y-yhat)/y)**2\n",
    "    return (np.mean(inner))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rossmann, store, & test csv files as a DataFrame\n",
    "train_df  = pd.read_csv(\"../input/train.csv\")\n",
    "store_df     = pd.read_csv(\"../input/store.csv\")\n",
    "test_df      = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Got a warning about column 7 during import.\n",
    "# Fix it by change all 0 to '0'\n",
    "train_df.loc[train_df['StateHoliday'] == 0, 'StateHoliday'] = '0'\n",
    "\n",
    "# Convert the date column in train and test data\n",
    "train_df['Date'] = pd.to_datetime(train_df['Date'], format=\"%Y-%m-%d\")\n",
    "test_df['Date'] = pd.to_datetime(test_df['Date'], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Keep only records where the store is open\n",
    "train_df = train_df[train_df['Open'] == 1]\n",
    "\n",
    "# Keep only records with non-zero sales\n",
    "train_df = train_df[train_df['Sales'] > 0]\n",
    "\n",
    "# !!!!!!!\n",
    "train_df.drop('StateHoliday', inplace=True, axis=1)\n",
    "train_df.drop('SchoolHoliday', inplace=True, axis=1)\n",
    "test_df.drop('StateHoliday', inplace=True, axis=1)\n",
    "test_df.drop('SchoolHoliday', inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convertColToCategory(data, colName, knownCategories):\n",
    "    data.loc[:, colName] = data.loc[:, colName].astype('category', categories=knownCategories)\n",
    "\n",
    "def transformData(inputData):\n",
    "\n",
    "    data = inputData.copy()\n",
    "    startDate = train_df['Date'].min()\n",
    "\n",
    "    # StateHoliday have values of both '0' and 0.  Change all 0 to '0'\n",
    "    #data.loc[data['StateHoliday'] == 0, ('StateHoliday')] = '0'\n",
    "\n",
    "    #data.loc[:, ('daysSince')] = data['Date'].apply(lambda d: (d - startDate).days)\n",
    "\n",
    "    # Convert categorical columns to category type\n",
    "    convertColToCategory(data, 'DayOfWeek', range(1, 8))\n",
    "    convertColToCategory(data, 'Promo', range(2))\n",
    "    #convertColToCategory(data, 'SchoolHoliday', range(2))\n",
    "    #convertColToCategory(data, 'StateHoliday', list('0abc'))\n",
    "    \n",
    "    \n",
    "    #if 'StoreType' in data.columns:\n",
    "    #    convertColToCategory(data, 'StoreType', list('abcd'))\n",
    "                         \n",
    "    #if 'Assortment' in data.columns:\n",
    "    #    convertColToCategory(data, 'Assortment', list('abc'))\n",
    "\n",
    "    return pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_full_df = pd.merge(train_df, store_df.ix[:,['StoreType', 'Assortment', 'Store']], on='Store')\n",
    "train_full_df = train_df\n",
    "train_data = transformData(train_full_df)\n",
    "\n",
    "train_data.set_index('Date', inplace=True)\n",
    "train_data.sort_index(inplace=True)\n",
    "\n",
    "#test_full_df = pd.merge(test_df, store_df.ix[:,['StoreType', 'Assortment', 'Store']], on='Store')\n",
    "test_full_df = test_df\n",
    "test_data = transformData(test_full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Store', u'Sales', u'Customers', u'Open', u'DayOfWeek_1',\n",
       "       u'DayOfWeek_2', u'DayOfWeek_3', u'DayOfWeek_4', u'DayOfWeek_5',\n",
       "       u'DayOfWeek_6', u'DayOfWeek_7', u'Promo_0', u'Promo_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns\n",
    "#print train_data.iloc[8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainModels(n_cutoff_months, algo, store=None, **kwargs):\n",
    "    all_y = []\n",
    "    all_yhat = []\n",
    "    models = {}\n",
    "    \n",
    "    if store is not None:\n",
    "        data = train_data[train_data['Store'] == store]\n",
    "    else:\n",
    "        data = train_data\n",
    "    \n",
    "    test_cutoff_date = data.index.max() - pd.DateOffset(months = n_cutoff_months)\n",
    "\n",
    "    train_set = data[data.index <= test_cutoff_date]\n",
    "    dev_set = data[data.index > test_cutoff_date]\n",
    "    print \"train_set size:\", len(train_set)\n",
    "    print \"dev_set size:\", len(dev_set)\n",
    "\n",
    "    \n",
    "    train_set_grouped = train_set.groupby('Store')\n",
    "    dev_set_grouped = dev_set.groupby('Store')\n",
    "    cols = None\n",
    "\n",
    "    for group in train_set_grouped.groups.keys():\n",
    "        x_drop_cols = ['Store', 'Sales', 'Customers', 'Open']\n",
    "\n",
    "        train_X = train_set_grouped.get_group(group).drop(x_drop_cols, axis=1, inplace=False)\n",
    "        train_y = np.log(train_set_grouped.get_group(group)['Sales'])\n",
    "\n",
    "        dev_X = dev_set_grouped.get_group(group).drop(x_drop_cols, axis=1, inplace=False)\n",
    "        dev_y = np.log(dev_set_grouped.get_group(group)['Sales'])\n",
    "\n",
    "        # One model per store\n",
    "        if algo == 'lr':\n",
    "            model = linear_model.LinearRegression(**kwargs)\n",
    "        elif algo == 'ridge':\n",
    "            model = linear_model.Ridge(**kwargs)\n",
    "        elif algo == 'svr':\n",
    "            model = svm.SVR(**kwargs)\n",
    "        elif algo == 'rfr':\n",
    "            model = ensemble.RandomForestRegressor(**kwargs)\n",
    "        elif algo == 'kr':\n",
    "            model = kernel_ridge.KernelRidge(**kwargs)\n",
    "            \n",
    "        model.fit(X=train_X, y=train_y)\n",
    "        models[group] = model\n",
    "        yhat = model.predict(dev_X)\n",
    "\n",
    "        all_y.extend(dev_y.tolist())\n",
    "        all_yhat.extend(yhat.tolist())\n",
    "        \n",
    "    print \"Training model - dev data RMSPE=\", rmspe(np.exp(np.array(all_y)), np.exp(np.array(all_yhat)))\n",
    "    print \"Model:\\n\", models[data.iloc[0]['Store']]\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set size: 785727\n",
      "dev_set size: 58611\n",
      "Training model - dev data RMSPE= 0.14634937911\n",
      "Model:\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n"
     ]
    }
   ],
   "source": [
    "storeid = None\n",
    "models = trainModels(2, 'lr', storeid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set size: 785727\n",
      "dev_set size: 58611\n",
      "Training model - dev data RMSPE= 0.146358709517\n",
      "Model:\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "models = trainModels(2, 'ridge', storeid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set size: 785727\n",
      "dev_set size: 58611\n",
      "Training model - dev data RMSPE= 0.14148880619\n",
      "Model:\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "models = trainModels(2, 'svr', storeid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set size: 785727\n",
      "dev_set size: 58611\n",
      "Training model - dev data RMSPE= 0.175688739913\n",
      "Model:\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=0.5, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=20, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "models = trainModels(2, 'rfr', n_jobs=-1, max_features=0.5, n_estimators=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions on Test data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makePredictions(data):\n",
    "    test_ids = []\n",
    "    test_yhats = []\n",
    "\n",
    "    grouped = data.groupby('Store')\n",
    "\n",
    "    for storeId, group in grouped:\n",
    "        x_drop_cols = ['Id', 'Open', 'Store', 'Date']\n",
    "\n",
    "        ids = group['Id']\n",
    "        X = group.drop(x_drop_cols, axis=1, inplace=False)\n",
    "\n",
    "        yhat = np.exp(models[storeId].predict(X))\n",
    "\n",
    "        # Ignore prediction and set Sales to zero hen the store is closed\n",
    "        yhat[np.array(group['Open'] == 0)] = 0\n",
    "        test_yhats += yhat.tolist()\n",
    "        test_ids += ids.tolist()\n",
    "\n",
    "    res = [[i, y] for i, y in zip(test_ids, test_yhats)]\n",
    "    return sorted(res, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = makePredictions(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"submission.csv\", \"w\")\n",
    "f.write('\"Id\",\"Sales\"\\n')\n",
    "\n",
    "for r in result:\n",
    "    f.write(\"%d,%d\\n\" % (r[0],int(r[1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc stuff ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def littleTest():\n",
    "    # Test prediction using some input generated from training data\n",
    "    random_idx = random.sample(train_df.index.tolist(), 100)\n",
    "\n",
    "    try_df = train_df.loc[random_idx]\n",
    "    try_y = try_df['Sales']\n",
    "    try_df.drop(['Sales', 'Customers'], axis=1, inplace=True)\n",
    "    try_df[\"Id\"] = range(1, len(try_df) + 1)\n",
    "    #try_data = transformData(pd.merge(try_df, store_df.ix[:,['StoreType', 'Assortment', 'Store']], on='Store'))\n",
    "    try_data = transformData(try_df)\n",
    "\n",
    "    result = makePredictions(try_data)\n",
    "    try_yhat = np.array([r[1] for r in result])\n",
    "\n",
    "    print rmspe(try_y, try_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998459903729\n"
     ]
    }
   ],
   "source": [
    "littleTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
