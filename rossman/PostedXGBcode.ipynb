{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the training, test and store data using pandas\n",
      "Assume store open, if not provided\n",
      "Consider only open stores for training. Closed stores wont count into the score.\n",
      "Use only Sales bigger then zero. Simplifies calculation of rmspe\n",
      "Join with store\n",
      "augment features\n",
      "['Store', 'CompetitionDistance', 'Promo', 'Promo2', 'SchoolHoliday', 'StoreType', 'Assortment', 'StateHoliday', 'DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear', 'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 100 rounds.\n",
      "[0]\ttrain-rmspe:0.996830\teval-rmspe:0.996828\n",
      "[1]\ttrain-rmspe:0.981414\teval-rmspe:0.981413\n",
      "[2]\ttrain-rmspe:0.937928\teval-rmspe:0.937948\n",
      "[3]\ttrain-rmspe:0.856420\teval-rmspe:0.856414\n",
      "[4]\ttrain-rmspe:0.743915\teval-rmspe:0.743456\n",
      "[5]\ttrain-rmspe:0.619839\teval-rmspe:0.618119\n",
      "[6]\ttrain-rmspe:0.505827\teval-rmspe:0.501297\n",
      "[7]\ttrain-rmspe:0.416382\teval-rmspe:0.407330\n",
      "[8]\ttrain-rmspe:0.352812\teval-rmspe:0.339692\n",
      "[9]\ttrain-rmspe:0.319409\teval-rmspe:0.302301\n",
      "[10]\ttrain-rmspe:0.298488\teval-rmspe:0.276217\n",
      "[11]\ttrain-rmspe:0.288719\teval-rmspe:0.263493\n",
      "[12]\ttrain-rmspe:0.280855\teval-rmspe:0.252323\n",
      "[13]\ttrain-rmspe:0.270708\teval-rmspe:0.241224\n",
      "[14]\ttrain-rmspe:0.260610\teval-rmspe:0.226547\n",
      "[15]\ttrain-rmspe:0.260467\teval-rmspe:0.226868\n",
      "[16]\ttrain-rmspe:0.249363\teval-rmspe:0.223905\n",
      "[17]\ttrain-rmspe:0.248612\teval-rmspe:0.223629\n",
      "[18]\ttrain-rmspe:0.236344\teval-rmspe:0.210989\n",
      "[19]\ttrain-rmspe:0.235437\teval-rmspe:0.209829\n",
      "[20]\ttrain-rmspe:0.234261\teval-rmspe:0.208455\n",
      "[21]\ttrain-rmspe:0.231225\teval-rmspe:0.205822\n",
      "[22]\ttrain-rmspe:0.226759\teval-rmspe:0.200788\n",
      "[23]\ttrain-rmspe:0.215586\teval-rmspe:0.198488\n",
      "[24]\ttrain-rmspe:0.213415\teval-rmspe:0.196869\n",
      "[25]\ttrain-rmspe:0.209429\teval-rmspe:0.192498\n",
      "[26]\ttrain-rmspe:0.207673\teval-rmspe:0.190692\n",
      "[27]\ttrain-rmspe:0.204381\teval-rmspe:0.187423\n",
      "[28]\ttrain-rmspe:0.200999\teval-rmspe:0.183543\n",
      "[29]\ttrain-rmspe:0.196413\teval-rmspe:0.179829\n",
      "[30]\ttrain-rmspe:0.193149\teval-rmspe:0.176060\n",
      "[31]\ttrain-rmspe:0.191006\teval-rmspe:0.173760\n",
      "[32]\ttrain-rmspe:0.187008\teval-rmspe:0.168227\n",
      "[33]\ttrain-rmspe:0.183291\teval-rmspe:0.164264\n",
      "[34]\ttrain-rmspe:0.182109\teval-rmspe:0.162967\n",
      "[35]\ttrain-rmspe:0.178245\teval-rmspe:0.161887\n",
      "[36]\ttrain-rmspe:0.175633\teval-rmspe:0.159780\n",
      "[37]\ttrain-rmspe:0.172119\teval-rmspe:0.155595\n",
      "[38]\ttrain-rmspe:0.169868\teval-rmspe:0.153613\n",
      "[39]\ttrain-rmspe:0.168127\teval-rmspe:0.152161\n",
      "[40]\ttrain-rmspe:0.166714\teval-rmspe:0.150861\n",
      "[41]\ttrain-rmspe:0.165392\teval-rmspe:0.149572\n",
      "[42]\ttrain-rmspe:0.164153\teval-rmspe:0.148377\n",
      "[43]\ttrain-rmspe:0.161522\teval-rmspe:0.145372\n",
      "[44]\ttrain-rmspe:0.160428\teval-rmspe:0.144058\n",
      "[45]\ttrain-rmspe:0.160064\teval-rmspe:0.143512\n",
      "[46]\ttrain-rmspe:0.159075\teval-rmspe:0.142632\n",
      "[47]\ttrain-rmspe:0.158694\teval-rmspe:0.142268\n",
      "[48]\ttrain-rmspe:0.158046\teval-rmspe:0.141708\n",
      "[49]\ttrain-rmspe:0.157272\teval-rmspe:0.140962\n",
      "[50]\ttrain-rmspe:0.155855\teval-rmspe:0.139244\n",
      "[51]\ttrain-rmspe:0.155496\teval-rmspe:0.138686\n",
      "[52]\ttrain-rmspe:0.154233\teval-rmspe:0.137967\n",
      "[53]\ttrain-rmspe:0.153984\teval-rmspe:0.137728\n",
      "[54]\ttrain-rmspe:0.152501\teval-rmspe:0.136120\n",
      "[55]\ttrain-rmspe:0.151405\teval-rmspe:0.135817\n",
      "[56]\ttrain-rmspe:0.150466\teval-rmspe:0.134581\n",
      "[57]\ttrain-rmspe:0.149443\teval-rmspe:0.133495\n",
      "[58]\ttrain-rmspe:0.148423\teval-rmspe:0.132193\n",
      "[59]\ttrain-rmspe:0.146076\teval-rmspe:0.129417\n",
      "[60]\ttrain-rmspe:0.144839\teval-rmspe:0.128530\n",
      "[61]\ttrain-rmspe:0.144707\teval-rmspe:0.128367\n",
      "[62]\ttrain-rmspe:0.143853\teval-rmspe:0.128041\n",
      "[63]\ttrain-rmspe:0.143082\teval-rmspe:0.127260\n",
      "[64]\ttrain-rmspe:0.142483\teval-rmspe:0.126720\n",
      "[65]\ttrain-rmspe:0.141862\teval-rmspe:0.126254\n",
      "[66]\ttrain-rmspe:0.141291\teval-rmspe:0.125548\n",
      "[67]\ttrain-rmspe:0.140799\teval-rmspe:0.125166\n",
      "[68]\ttrain-rmspe:0.140379\teval-rmspe:0.124585\n",
      "[69]\ttrain-rmspe:0.140183\teval-rmspe:0.124376\n",
      "[70]\ttrain-rmspe:0.139646\teval-rmspe:0.123705\n",
      "[71]\ttrain-rmspe:0.138714\teval-rmspe:0.122769\n",
      "[72]\ttrain-rmspe:0.137716\teval-rmspe:0.121916\n",
      "[73]\ttrain-rmspe:0.137272\teval-rmspe:0.121467\n",
      "[74]\ttrain-rmspe:0.136921\teval-rmspe:0.121335\n",
      "[75]\ttrain-rmspe:0.136328\teval-rmspe:0.120623\n",
      "[76]\ttrain-rmspe:0.135750\teval-rmspe:0.120091\n",
      "[77]\ttrain-rmspe:0.135304\teval-rmspe:0.119592\n",
      "[78]\ttrain-rmspe:0.134901\teval-rmspe:0.119282\n",
      "[79]\ttrain-rmspe:0.134431\teval-rmspe:0.118883\n",
      "[80]\ttrain-rmspe:0.134137\teval-rmspe:0.118629\n",
      "[81]\ttrain-rmspe:0.133865\teval-rmspe:0.118453\n",
      "[82]\ttrain-rmspe:0.133612\teval-rmspe:0.118336\n",
      "[83]\ttrain-rmspe:0.132927\teval-rmspe:0.117655\n",
      "[84]\ttrain-rmspe:0.132364\teval-rmspe:0.117410\n",
      "[85]\ttrain-rmspe:0.132118\teval-rmspe:0.117252\n",
      "[86]\ttrain-rmspe:0.131547\teval-rmspe:0.117016\n",
      "[87]\ttrain-rmspe:0.130548\teval-rmspe:0.115664\n",
      "[88]\ttrain-rmspe:0.130214\teval-rmspe:0.115259\n",
      "[89]\ttrain-rmspe:0.129861\teval-rmspe:0.114963\n",
      "[90]\ttrain-rmspe:0.129255\teval-rmspe:0.114372\n",
      "[91]\ttrain-rmspe:0.128615\teval-rmspe:0.113629\n",
      "[92]\ttrain-rmspe:0.128302\teval-rmspe:0.113337\n",
      "[93]\ttrain-rmspe:0.127557\teval-rmspe:0.112955\n",
      "[94]\ttrain-rmspe:0.127389\teval-rmspe:0.112719\n",
      "[95]\ttrain-rmspe:0.127020\teval-rmspe:0.112456\n",
      "[96]\ttrain-rmspe:0.126239\teval-rmspe:0.112166\n",
      "[97]\ttrain-rmspe:0.125883\teval-rmspe:0.111700\n",
      "[98]\ttrain-rmspe:0.125818\teval-rmspe:0.111609\n",
      "[99]\ttrain-rmspe:0.125592\teval-rmspe:0.111417\n",
      "[100]\ttrain-rmspe:0.125086\teval-rmspe:0.111054\n",
      "[101]\ttrain-rmspe:0.124473\teval-rmspe:0.110478\n",
      "[102]\ttrain-rmspe:0.124355\teval-rmspe:0.110398\n",
      "[103]\ttrain-rmspe:0.124150\teval-rmspe:0.110222\n",
      "[104]\ttrain-rmspe:0.124007\teval-rmspe:0.110107\n",
      "[105]\ttrain-rmspe:0.123760\teval-rmspe:0.109728\n",
      "[106]\ttrain-rmspe:0.123704\teval-rmspe:0.109681\n",
      "[107]\ttrain-rmspe:0.123474\teval-rmspe:0.109569\n",
      "[108]\ttrain-rmspe:0.123246\teval-rmspe:0.109393\n",
      "[109]\ttrain-rmspe:0.122930\teval-rmspe:0.109129\n",
      "[110]\ttrain-rmspe:0.122699\teval-rmspe:0.108897\n",
      "[111]\ttrain-rmspe:0.122414\teval-rmspe:0.108836\n",
      "[112]\ttrain-rmspe:0.121909\teval-rmspe:0.108578\n",
      "[113]\ttrain-rmspe:0.121860\teval-rmspe:0.108548\n",
      "[114]\ttrain-rmspe:0.121417\teval-rmspe:0.108445\n",
      "[115]\ttrain-rmspe:0.121134\teval-rmspe:0.108201\n",
      "[116]\ttrain-rmspe:0.120950\teval-rmspe:0.108068\n",
      "[117]\ttrain-rmspe:0.120720\teval-rmspe:0.107823\n",
      "[118]\ttrain-rmspe:0.120560\teval-rmspe:0.107681\n",
      "[119]\ttrain-rmspe:0.120369\teval-rmspe:0.107536\n",
      "[120]\ttrain-rmspe:0.120206\teval-rmspe:0.107492\n",
      "[121]\ttrain-rmspe:0.120034\teval-rmspe:0.107347\n",
      "[122]\ttrain-rmspe:0.119842\teval-rmspe:0.107229\n",
      "[123]\ttrain-rmspe:0.119681\teval-rmspe:0.106973\n",
      "[124]\ttrain-rmspe:0.119440\teval-rmspe:0.106879\n",
      "[125]\ttrain-rmspe:0.119112\teval-rmspe:0.106522\n",
      "[126]\ttrain-rmspe:0.118994\teval-rmspe:0.106407\n",
      "[127]\ttrain-rmspe:0.118798\teval-rmspe:0.106154\n",
      "[128]\ttrain-rmspe:0.118582\teval-rmspe:0.105984\n",
      "[129]\ttrain-rmspe:0.118323\teval-rmspe:0.105844\n",
      "[130]\ttrain-rmspe:0.118145\teval-rmspe:0.105669\n",
      "[131]\ttrain-rmspe:0.117760\teval-rmspe:0.105351\n",
      "[132]\ttrain-rmspe:0.117613\teval-rmspe:0.105241\n",
      "[133]\ttrain-rmspe:0.117201\teval-rmspe:0.104995\n",
      "[134]\ttrain-rmspe:0.116013\teval-rmspe:0.104842\n",
      "[135]\ttrain-rmspe:0.115853\teval-rmspe:0.104787\n",
      "[136]\ttrain-rmspe:0.115025\teval-rmspe:0.104662\n",
      "[137]\ttrain-rmspe:0.114690\teval-rmspe:0.104525\n",
      "[138]\ttrain-rmspe:0.114450\teval-rmspe:0.104370\n",
      "[139]\ttrain-rmspe:0.114272\teval-rmspe:0.104224\n",
      "[140]\ttrain-rmspe:0.114049\teval-rmspe:0.103747\n",
      "[141]\ttrain-rmspe:0.114003\teval-rmspe:0.103725\n",
      "[142]\ttrain-rmspe:0.113858\teval-rmspe:0.103588\n",
      "[143]\ttrain-rmspe:0.113790\teval-rmspe:0.103526\n",
      "[144]\ttrain-rmspe:0.113613\teval-rmspe:0.103460\n",
      "[145]\ttrain-rmspe:0.113202\teval-rmspe:0.103367\n",
      "[146]\ttrain-rmspe:0.113015\teval-rmspe:0.103291\n",
      "[147]\ttrain-rmspe:0.112811\teval-rmspe:0.103143\n",
      "[148]\ttrain-rmspe:0.112744\teval-rmspe:0.103110\n",
      "[149]\ttrain-rmspe:0.112575\teval-rmspe:0.103055\n",
      "[150]\ttrain-rmspe:0.112357\teval-rmspe:0.102895\n",
      "[151]\ttrain-rmspe:0.112254\teval-rmspe:0.102781\n",
      "[152]\ttrain-rmspe:0.112120\teval-rmspe:0.102656\n",
      "[153]\ttrain-rmspe:0.111822\teval-rmspe:0.102438\n",
      "[154]\ttrain-rmspe:0.111724\teval-rmspe:0.102361\n",
      "[155]\ttrain-rmspe:0.111620\teval-rmspe:0.102228\n",
      "[156]\ttrain-rmspe:0.111116\teval-rmspe:0.102029\n",
      "[157]\ttrain-rmspe:0.110998\teval-rmspe:0.101968\n",
      "[158]\ttrain-rmspe:0.110864\teval-rmspe:0.101919\n",
      "[159]\ttrain-rmspe:0.110765\teval-rmspe:0.101843\n",
      "[160]\ttrain-rmspe:0.110418\teval-rmspe:0.101797\n",
      "[161]\ttrain-rmspe:0.110313\teval-rmspe:0.101767\n",
      "[162]\ttrain-rmspe:0.110196\teval-rmspe:0.101764\n",
      "[163]\ttrain-rmspe:0.110073\teval-rmspe:0.101694\n",
      "[164]\ttrain-rmspe:0.109995\teval-rmspe:0.101622\n",
      "[165]\ttrain-rmspe:0.109826\teval-rmspe:0.101497\n",
      "[166]\ttrain-rmspe:0.109686\teval-rmspe:0.101427\n",
      "[167]\ttrain-rmspe:0.109496\teval-rmspe:0.101432\n",
      "[168]\ttrain-rmspe:0.109400\teval-rmspe:0.101299\n",
      "[169]\ttrain-rmspe:0.109010\teval-rmspe:0.101158\n",
      "[170]\ttrain-rmspe:0.108879\teval-rmspe:0.101133\n",
      "[171]\ttrain-rmspe:0.108789\teval-rmspe:0.101069\n",
      "[172]\ttrain-rmspe:0.108684\teval-rmspe:0.101058\n",
      "[173]\ttrain-rmspe:0.108537\teval-rmspe:0.100886\n",
      "[174]\ttrain-rmspe:0.108654\teval-rmspe:0.100765\n",
      "[175]\ttrain-rmspe:0.108508\teval-rmspe:0.100626\n",
      "[176]\ttrain-rmspe:0.108451\teval-rmspe:0.100591\n",
      "[177]\ttrain-rmspe:0.108330\teval-rmspe:0.100515\n",
      "[178]\ttrain-rmspe:0.108201\teval-rmspe:0.100444\n",
      "[179]\ttrain-rmspe:0.108059\teval-rmspe:0.100377\n",
      "[180]\ttrain-rmspe:0.108006\teval-rmspe:0.100373\n",
      "[181]\ttrain-rmspe:0.107961\teval-rmspe:0.100323\n",
      "[182]\ttrain-rmspe:0.107814\teval-rmspe:0.100201\n",
      "[183]\ttrain-rmspe:0.107671\teval-rmspe:0.100160\n",
      "[184]\ttrain-rmspe:0.107518\teval-rmspe:0.100122\n",
      "[185]\ttrain-rmspe:0.107409\teval-rmspe:0.100053\n",
      "[186]\ttrain-rmspe:0.107296\teval-rmspe:0.100014\n",
      "[187]\ttrain-rmspe:0.107241\teval-rmspe:0.099979\n",
      "[188]\ttrain-rmspe:0.107075\teval-rmspe:0.099864\n",
      "[189]\ttrain-rmspe:0.106968\teval-rmspe:0.099878\n",
      "[190]\ttrain-rmspe:0.106879\teval-rmspe:0.099795\n",
      "[191]\ttrain-rmspe:0.106758\teval-rmspe:0.099724\n",
      "[192]\ttrain-rmspe:0.106652\teval-rmspe:0.099680\n",
      "[193]\ttrain-rmspe:0.106429\teval-rmspe:0.099612\n",
      "[194]\ttrain-rmspe:0.106383\teval-rmspe:0.099590\n",
      "[195]\ttrain-rmspe:0.101235\teval-rmspe:0.099553\n",
      "[196]\ttrain-rmspe:0.101105\teval-rmspe:0.099562\n",
      "[197]\ttrain-rmspe:0.100958\teval-rmspe:0.099441\n",
      "[198]\ttrain-rmspe:0.100876\teval-rmspe:0.099377\n",
      "[199]\ttrain-rmspe:0.100839\teval-rmspe:0.099357\n",
      "[200]\ttrain-rmspe:0.100740\teval-rmspe:0.099323\n",
      "[201]\ttrain-rmspe:0.100533\teval-rmspe:0.099278\n",
      "[202]\ttrain-rmspe:0.100422\teval-rmspe:0.099232\n",
      "[203]\ttrain-rmspe:0.100352\teval-rmspe:0.099204\n",
      "[204]\ttrain-rmspe:0.100263\teval-rmspe:0.099170\n",
      "[205]\ttrain-rmspe:0.100171\teval-rmspe:0.099080\n",
      "[206]\ttrain-rmspe:0.100081\teval-rmspe:0.099079\n",
      "[207]\ttrain-rmspe:0.099949\teval-rmspe:0.099054\n",
      "[208]\ttrain-rmspe:0.099897\teval-rmspe:0.098972\n",
      "[209]\ttrain-rmspe:0.099816\teval-rmspe:0.098956\n",
      "[210]\ttrain-rmspe:0.099714\teval-rmspe:0.098910\n",
      "[211]\ttrain-rmspe:0.099631\teval-rmspe:0.098758\n",
      "[212]\ttrain-rmspe:0.099580\teval-rmspe:0.098743\n",
      "[213]\ttrain-rmspe:0.099564\teval-rmspe:0.098755\n",
      "[214]\ttrain-rmspe:0.099473\teval-rmspe:0.098752\n",
      "[215]\ttrain-rmspe:0.099351\teval-rmspe:0.098697\n",
      "[216]\ttrain-rmspe:0.099165\teval-rmspe:0.098632\n",
      "[217]\ttrain-rmspe:0.098996\teval-rmspe:0.098526\n",
      "[218]\ttrain-rmspe:0.098882\teval-rmspe:0.098498\n",
      "[219]\ttrain-rmspe:0.098767\teval-rmspe:0.098433\n",
      "[220]\ttrain-rmspe:0.098134\teval-rmspe:0.098417\n",
      "[221]\ttrain-rmspe:0.098096\teval-rmspe:0.098413\n",
      "[222]\ttrain-rmspe:0.097987\teval-rmspe:0.098353\n",
      "[223]\ttrain-rmspe:0.097915\teval-rmspe:0.098277\n",
      "[224]\ttrain-rmspe:0.097813\teval-rmspe:0.098248\n",
      "[225]\ttrain-rmspe:0.097707\teval-rmspe:0.098199\n",
      "[226]\ttrain-rmspe:0.097576\teval-rmspe:0.098090\n",
      "[227]\ttrain-rmspe:0.097516\teval-rmspe:0.098009\n",
      "[228]\ttrain-rmspe:0.097398\teval-rmspe:0.097974\n",
      "[229]\ttrain-rmspe:0.097349\teval-rmspe:0.097945\n",
      "[230]\ttrain-rmspe:0.097179\teval-rmspe:0.097843\n",
      "[231]\ttrain-rmspe:0.094855\teval-rmspe:0.097788\n",
      "[232]\ttrain-rmspe:0.094769\teval-rmspe:0.097774\n",
      "[233]\ttrain-rmspe:0.094681\teval-rmspe:0.097746\n",
      "[234]\ttrain-rmspe:0.094602\teval-rmspe:0.097693\n",
      "[235]\ttrain-rmspe:0.094467\teval-rmspe:0.097692\n",
      "[236]\ttrain-rmspe:0.094343\teval-rmspe:0.097660\n",
      "[237]\ttrain-rmspe:0.094177\teval-rmspe:0.097585\n",
      "[238]\ttrain-rmspe:0.094088\teval-rmspe:0.097573\n",
      "[239]\ttrain-rmspe:0.093987\teval-rmspe:0.097550\n",
      "[240]\ttrain-rmspe:0.093940\teval-rmspe:0.097540\n",
      "[241]\ttrain-rmspe:0.093885\teval-rmspe:0.097509\n",
      "[242]\ttrain-rmspe:0.093757\teval-rmspe:0.097478\n",
      "[243]\ttrain-rmspe:0.093663\teval-rmspe:0.097431\n",
      "[244]\ttrain-rmspe:0.093576\teval-rmspe:0.097400\n",
      "[245]\ttrain-rmspe:0.093488\teval-rmspe:0.097371\n",
      "[246]\ttrain-rmspe:0.091328\teval-rmspe:0.097382\n",
      "[247]\ttrain-rmspe:0.091198\teval-rmspe:0.097297\n",
      "[248]\ttrain-rmspe:0.091120\teval-rmspe:0.097267\n",
      "[249]\ttrain-rmspe:0.090878\teval-rmspe:0.097229\n",
      "[250]\ttrain-rmspe:0.090765\teval-rmspe:0.097201\n",
      "[251]\ttrain-rmspe:0.090667\teval-rmspe:0.097167\n",
      "[252]\ttrain-rmspe:0.090612\teval-rmspe:0.097158\n",
      "[253]\ttrain-rmspe:0.090492\teval-rmspe:0.097028\n",
      "[254]\ttrain-rmspe:0.090322\teval-rmspe:0.096945\n",
      "[255]\ttrain-rmspe:0.090193\teval-rmspe:0.096912\n",
      "[256]\ttrain-rmspe:0.090129\teval-rmspe:0.096906\n",
      "[257]\ttrain-rmspe:0.090024\teval-rmspe:0.096803\n",
      "[258]\ttrain-rmspe:0.089938\teval-rmspe:0.096783\n",
      "[259]\ttrain-rmspe:0.089877\teval-rmspe:0.096749\n",
      "[260]\ttrain-rmspe:0.089803\teval-rmspe:0.096736\n",
      "[261]\ttrain-rmspe:0.089721\teval-rmspe:0.096680\n",
      "[262]\ttrain-rmspe:0.089685\teval-rmspe:0.096682\n",
      "[263]\ttrain-rmspe:0.089349\teval-rmspe:0.096660\n",
      "[264]\ttrain-rmspe:0.087859\teval-rmspe:0.096626\n",
      "[265]\ttrain-rmspe:0.087805\teval-rmspe:0.096641\n",
      "[266]\ttrain-rmspe:0.087733\teval-rmspe:0.096631\n",
      "[267]\ttrain-rmspe:0.084706\teval-rmspe:0.096599\n",
      "[268]\ttrain-rmspe:0.084558\teval-rmspe:0.096566\n",
      "[269]\ttrain-rmspe:0.084395\teval-rmspe:0.096485\n",
      "[270]\ttrain-rmspe:0.084295\teval-rmspe:0.096409\n",
      "[271]\ttrain-rmspe:0.084225\teval-rmspe:0.096379\n",
      "[272]\ttrain-rmspe:0.084166\teval-rmspe:0.096345\n",
      "[273]\ttrain-rmspe:0.084080\teval-rmspe:0.096330\n",
      "[274]\ttrain-rmspe:0.083547\teval-rmspe:0.096259\n",
      "[275]\ttrain-rmspe:0.083449\teval-rmspe:0.096198\n",
      "[276]\ttrain-rmspe:0.083364\teval-rmspe:0.096188\n",
      "[277]\ttrain-rmspe:0.083279\teval-rmspe:0.096150\n",
      "[278]\ttrain-rmspe:0.083142\teval-rmspe:0.096089\n",
      "[279]\ttrain-rmspe:0.083041\teval-rmspe:0.096035\n",
      "[280]\ttrain-rmspe:0.082991\teval-rmspe:0.096030\n",
      "[281]\ttrain-rmspe:0.082932\teval-rmspe:0.096024\n",
      "[282]\ttrain-rmspe:0.082857\teval-rmspe:0.096007\n",
      "[283]\ttrain-rmspe:0.082764\teval-rmspe:0.095990\n",
      "[284]\ttrain-rmspe:0.082611\teval-rmspe:0.095969\n",
      "[285]\ttrain-rmspe:0.082307\teval-rmspe:0.095952\n",
      "[286]\ttrain-rmspe:0.082231\teval-rmspe:0.095941\n",
      "[287]\ttrain-rmspe:0.082098\teval-rmspe:0.095922\n",
      "[288]\ttrain-rmspe:0.082035\teval-rmspe:0.095899\n",
      "[289]\ttrain-rmspe:0.081963\teval-rmspe:0.095885\n",
      "[290]\ttrain-rmspe:0.081935\teval-rmspe:0.095865\n",
      "[291]\ttrain-rmspe:0.081873\teval-rmspe:0.095826\n",
      "[292]\ttrain-rmspe:0.081825\teval-rmspe:0.095817\n",
      "[293]\ttrain-rmspe:0.081717\teval-rmspe:0.095744\n",
      "[294]\ttrain-rmspe:0.081638\teval-rmspe:0.095735\n",
      "[295]\ttrain-rmspe:0.081891\teval-rmspe:0.095693\n",
      "[296]\ttrain-rmspe:0.081828\teval-rmspe:0.095667\n",
      "[297]\ttrain-rmspe:0.081786\teval-rmspe:0.095644\n",
      "[298]\ttrain-rmspe:0.081731\teval-rmspe:0.095631\n",
      "[299]\ttrain-rmspe:0.081673\teval-rmspe:0.095589\n",
      "[300]\ttrain-rmspe:0.081635\teval-rmspe:0.095576\n",
      "[301]\ttrain-rmspe:0.081606\teval-rmspe:0.095563\n",
      "[302]\ttrain-rmspe:0.081544\teval-rmspe:0.095553\n",
      "[303]\ttrain-rmspe:0.081492\teval-rmspe:0.095546\n",
      "[304]\ttrain-rmspe:0.081451\teval-rmspe:0.095526\n",
      "[305]\ttrain-rmspe:0.081288\teval-rmspe:0.095538\n",
      "[306]\ttrain-rmspe:0.081232\teval-rmspe:0.095525\n",
      "[307]\ttrain-rmspe:0.081040\teval-rmspe:0.095533\n",
      "[308]\ttrain-rmspe:0.081000\teval-rmspe:0.095506\n",
      "[309]\ttrain-rmspe:0.080973\teval-rmspe:0.095487\n",
      "[310]\ttrain-rmspe:0.080937\teval-rmspe:0.095477\n",
      "[311]\ttrain-rmspe:0.080905\teval-rmspe:0.095467\n",
      "[312]\ttrain-rmspe:0.080826\teval-rmspe:0.095472\n",
      "[313]\ttrain-rmspe:0.080775\teval-rmspe:0.095421\n",
      "[314]\ttrain-rmspe:0.080657\teval-rmspe:0.095407\n",
      "[315]\ttrain-rmspe:0.080578\teval-rmspe:0.095394\n",
      "[316]\ttrain-rmspe:0.080542\teval-rmspe:0.095387\n",
      "[317]\ttrain-rmspe:0.080490\teval-rmspe:0.095372\n",
      "[318]\ttrain-rmspe:0.080435\teval-rmspe:0.095377\n",
      "[319]\ttrain-rmspe:0.080392\teval-rmspe:0.095373\n",
      "[320]\ttrain-rmspe:0.080331\teval-rmspe:0.095345\n",
      "[321]\ttrain-rmspe:0.080226\teval-rmspe:0.095346\n",
      "[322]\ttrain-rmspe:0.078892\teval-rmspe:0.095308\n",
      "[323]\ttrain-rmspe:0.078621\teval-rmspe:0.095306\n",
      "[324]\ttrain-rmspe:0.078587\teval-rmspe:0.095304\n",
      "[325]\ttrain-rmspe:0.078519\teval-rmspe:0.095303\n",
      "[326]\ttrain-rmspe:0.078224\teval-rmspe:0.095268\n",
      "[327]\ttrain-rmspe:0.078138\teval-rmspe:0.095241\n",
      "[328]\ttrain-rmspe:0.078040\teval-rmspe:0.095260\n",
      "[329]\ttrain-rmspe:0.077991\teval-rmspe:0.095228\n",
      "[330]\ttrain-rmspe:0.077944\teval-rmspe:0.095235\n",
      "[331]\ttrain-rmspe:0.077839\teval-rmspe:0.095173\n",
      "[332]\ttrain-rmspe:0.077763\teval-rmspe:0.095138\n",
      "[333]\ttrain-rmspe:0.077711\teval-rmspe:0.095089\n",
      "[334]\ttrain-rmspe:0.077592\teval-rmspe:0.095071\n",
      "[335]\ttrain-rmspe:0.077517\teval-rmspe:0.095078\n",
      "[336]\ttrain-rmspe:0.077447\teval-rmspe:0.095082\n",
      "[337]\ttrain-rmspe:0.077361\teval-rmspe:0.094998\n",
      "[338]\ttrain-rmspe:0.077318\teval-rmspe:0.095002\n",
      "[339]\ttrain-rmspe:0.077238\teval-rmspe:0.095007\n",
      "[340]\ttrain-rmspe:0.077176\teval-rmspe:0.094971\n",
      "[341]\ttrain-rmspe:0.077103\teval-rmspe:0.094970\n",
      "[342]\ttrain-rmspe:0.077072\teval-rmspe:0.094961\n",
      "[343]\ttrain-rmspe:0.076974\teval-rmspe:0.094935\n",
      "[344]\ttrain-rmspe:0.076924\teval-rmspe:0.094893\n",
      "[345]\ttrain-rmspe:0.076815\teval-rmspe:0.094895\n",
      "[346]\ttrain-rmspe:0.076755\teval-rmspe:0.094841\n",
      "[347]\ttrain-rmspe:0.076693\teval-rmspe:0.094830\n",
      "[348]\ttrain-rmspe:0.076620\teval-rmspe:0.094811\n",
      "[349]\ttrain-rmspe:0.076587\teval-rmspe:0.094783\n",
      "[350]\ttrain-rmspe:0.076515\teval-rmspe:0.094750\n",
      "[351]\ttrain-rmspe:0.076462\teval-rmspe:0.094701\n",
      "[352]\ttrain-rmspe:0.076397\teval-rmspe:0.094672\n",
      "[353]\ttrain-rmspe:0.076347\teval-rmspe:0.094652\n",
      "[354]\ttrain-rmspe:0.076262\teval-rmspe:0.094623\n",
      "[355]\ttrain-rmspe:0.076215\teval-rmspe:0.094602\n",
      "[356]\ttrain-rmspe:0.076170\teval-rmspe:0.094611\n",
      "[357]\ttrain-rmspe:0.076120\teval-rmspe:0.094609\n",
      "[358]\ttrain-rmspe:0.076034\teval-rmspe:0.094584\n",
      "[359]\ttrain-rmspe:0.075997\teval-rmspe:0.094543\n",
      "[360]\ttrain-rmspe:0.075933\teval-rmspe:0.094527\n",
      "[361]\ttrain-rmspe:0.075882\teval-rmspe:0.094518\n",
      "[362]\ttrain-rmspe:0.075759\teval-rmspe:0.094481\n",
      "[363]\ttrain-rmspe:0.075695\teval-rmspe:0.094485\n",
      "[364]\ttrain-rmspe:0.075665\teval-rmspe:0.094464\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "'''\n",
    "Based on https://www.kaggle.com/cast42/rossmann-store-sales/xgboost-in-python-with-rmspe-v2\n",
    "which was based on\n",
    "Based on https://www.kaggle.com/justdoit/rossmann-store-sales/xgboost-in-python-with-rmspe/code\n",
    "Public Score :  0.11389\n",
    "Private Validation Score :  0.096959\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "import operator\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\") #Needed to save figures\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "\n",
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat/y-1) ** 2))\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat)\n",
    "\n",
    "# Gather some features\n",
    "def build_features(features, data):\n",
    "    # remove NaNs\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.loc[data.Open.isnull(), 'Open'] = 1\n",
    "    # Use some properties directly\n",
    "    features.extend(['Store', 'CompetitionDistance', 'Promo', 'Promo2', 'SchoolHoliday'])\n",
    "\n",
    "    # Label encode some features\n",
    "    features.extend(['StoreType', 'Assortment', 'StateHoliday'])\n",
    "    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "    data.StoreType.replace(mappings, inplace=True)\n",
    "    data.Assortment.replace(mappings, inplace=True)\n",
    "    data.StateHoliday.replace(mappings, inplace=True)\n",
    "\n",
    "    features.extend(['DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear'])\n",
    "    data['Year'] = data.Date.dt.year\n",
    "    data['Month'] = data.Date.dt.month\n",
    "    data['Day'] = data.Date.dt.day\n",
    "    data['DayOfWeek'] = data.Date.dt.dayofweek\n",
    "    data['WeekOfYear'] = data.Date.dt.weekofyear\n",
    "\n",
    "    # CompetionOpen en PromoOpen from https://www.kaggle.com/ananya77041/rossmann-store-sales/randomforestpython/code\n",
    "    # Calculate time competition open time in months\n",
    "    features.append('CompetitionOpen')\n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + \\\n",
    "        (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    # Promo open time in months\n",
    "    features.append('PromoOpen')\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + \\\n",
    "        (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n",
    "    data['PromoOpen'] = data.PromoOpen.apply(lambda x: x if x > 0 else 0)\n",
    "    data.loc[data.Promo2SinceYear == 0, 'PromoOpen'] = 0\n",
    "\n",
    "    # Indicate that sales on that day are in promo interval\n",
    "    features.append('IsPromoMonth')\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \\\n",
    "             7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    data['monthStr'] = data.Month.map(month2str)\n",
    "    data.loc[data.PromoInterval == 0, 'PromoInterval'] = ''\n",
    "    data['IsPromoMonth'] = 0\n",
    "    for interval in data.PromoInterval.unique():\n",
    "        if interval != '':\n",
    "            for month in interval.split(','):\n",
    "                data.loc[(data.monthStr == month) & (data.PromoInterval == interval), 'IsPromoMonth'] = 1\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "## Start of main script\n",
    "\n",
    "print(\"Load the training, test and store data using pandas\")\n",
    "types = {'CompetitionOpenSinceYear': np.dtype(int),\n",
    "         'CompetitionOpenSinceMonth': np.dtype(int),\n",
    "         'StateHoliday': np.dtype(str),\n",
    "         'Promo2SinceWeek': np.dtype(int),\n",
    "         'SchoolHoliday': np.dtype(float),\n",
    "         'PromoInterval': np.dtype(str)}\n",
    "train = pd.read_csv(\"train.csv\", parse_dates=[2], dtype=types)\n",
    "test = pd.read_csv(\"test.csv\", parse_dates=[3], dtype=types)\n",
    "store = pd.read_csv(\"store.csv\")\n",
    "\n",
    "print(\"Assume store open, if not provided\")\n",
    "train.fillna(1, inplace=True)\n",
    "test.fillna(1, inplace=True)\n",
    "\n",
    "print(\"Consider only open stores for training. Closed stores wont count into the score.\")\n",
    "train = train[train[\"Open\"] != 0]\n",
    "print(\"Use only Sales bigger then zero. Simplifies calculation of rmspe\")\n",
    "train = train[train[\"Sales\"] > 0]\n",
    "\n",
    "print(\"Join with store\")\n",
    "train = pd.merge(train, store, on='Store')\n",
    "test = pd.merge(test, store, on='Store')\n",
    "\n",
    "features = []\n",
    "\n",
    "print(\"augment features\")\n",
    "build_features(features, train)\n",
    "build_features([], test)\n",
    "print(features)\n",
    "\n",
    "print('training data processed')\n",
    "\n",
    "params = {\"objective\": \"reg:linear\",\n",
    "          \"booster\" : \"gbtree\",\n",
    "          \"eta\": 0.3,\n",
    "          \"max_depth\": 10,\n",
    "          \"subsample\": 0.9,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"silent\": 1,\n",
    "          \"seed\": 1301\n",
    "          }\n",
    "num_boost_round = 1000\n",
    "\n",
    "print(\"Train a XGBoost model\")\n",
    "X_train, X_valid = train_test_split(train, test_size=0.012, random_state=10)\n",
    "y_train = np.log1p(X_train.Sales)\n",
    "y_valid = np.log1p(X_valid.Sales)\n",
    "dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "  early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)\n",
    "\n",
    "print(\"Validating\")\n",
    "yhat = gbm.predict(xgb.DMatrix(X_valid[features]))\n",
    "error = rmspe(X_valid.Sales.values, np.expm1(yhat))\n",
    "print('RMSPE: {:.6f}'.format(error))\n",
    "\n",
    "print(\"Make predictions on the test set\")\n",
    "dtest = xgb.DMatrix(test[features])\n",
    "test_probs = gbm.predict(dtest)\n",
    "# Make Submission\n",
    "result = pd.DataFrame({\"Id\": test[\"Id\"], 'Sales': np.expm1(test_probs)})\n",
    "result.to_csv(\"xgboost_10_submission.csv\", index=False)\n",
    "\n",
    "# XGB feature importances\n",
    "# Based on https://www.kaggle.com/mmueller/liberty-mutual-group-property-inspection-prediction/xgb-feature-importance-python/code\n",
    "\n",
    "create_feature_map(features)\n",
    "importance = gbm.get_fscore(fmap='xgb.fmap')\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "df['fscore'] = df['fscore'] / df['fscore'].sum()\n",
    "\n",
    "featp = df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('relative importance')\n",
    "fig_featp = featp.get_figure()\n",
    "fig_featp.savefig('feature_importance_xgb.png', bbox_inches='tight', pad_inches=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
